<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distributed Systems on Zea&#39;s</title>
    <link>/categories/distributed-systems/</link>
    <description>Recent content in Distributed Systems on Zea&#39;s</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 09 Apr 2014 22:01:37 +0800</lastBuildDate>
    <atom:link href="/categories/distributed-systems/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>6.824 Lab1 MapReduce Notes</title>
      <link>/post/2014-04-09-6-dot-824-mapreduce-notes/</link>
      <pubDate>Wed, 09 Apr 2014 22:01:37 +0800</pubDate>
      
      <guid>/post/2014-04-09-6-dot-824-mapreduce-notes/</guid>
      <description>

&lt;p&gt;作为 MIT 6.824 &lt;em&gt;分布式系统&lt;/em&gt; 这门课的第一个 Lab，其主要的目的是让学生熟悉下 Go 语言，因此不是特别难。事实上，2013 年的 Lab1 是做一个 P/B 架构的 LockService，感觉今年的这个 MapReduce 难度上有所下降，更加适合上手，而内容方面涉及了 MapReduce 的简单应用和实现，做完之后收获更大，总的来说整体水准高于去年的 LockService。&lt;/p&gt;

&lt;p&gt;Lab 共分为三个部分，Part I 要求在 MapReduce 上实现一个 WordCount，因为大部分的 MapReduce 框架都已经实现好了，所以这部分非常简单；支持代码（ Support code ）中的 MapReduce 是个非常简化的版本，没有 Master，顺序执行每一个 Map 和 Reduce 操作，Part II 的要求是实现一个简单的 Master； Part III 要在 Part II 的基础之上实现容忍客户端失败的情况。&lt;/p&gt;

&lt;p&gt;总的来讲，这个 Lab 中我们只需要实现一个简化版的 MapReduce 中的 Master Node 的逻辑部分。&lt;/p&gt;

&lt;h3 id=&#34;part-i:9d64eb79111af9673b332797623c91f4&#34;&gt;Part I&lt;/h3&gt;

&lt;p&gt;只有一点点代码要写，不过在动手之前还是要认真阅读支持代码，主要需要搞明白的是 Map 和 Reduce 函数的输入输出分别是什么：&lt;/p&gt;

&lt;p&gt;``` go wc.go
func Map(value string) *list.List {}&lt;/p&gt;

&lt;p&gt;func Reduce(key string, values *list.List) string {}
```&lt;/p&gt;

&lt;p&gt;为此需要看一看 mapreduce.go 中 DoMap 和 DoReduce 这两个方法。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>6.824-lab3-notes</title>
      <link>/post/2014-02-21-6-dot-824-lab3-notes/</link>
      <pubDate>Fri, 21 Feb 2014 14:08:02 +0800</pubDate>
      
      <guid>/post/2014-02-21-6-dot-824-lab3-notes/</guid>
      <description>

&lt;h2 id=&#34;pre-read:6c805deaca0efd9342e0670f4a49bd04&#34;&gt;Pre-read&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://pdos.csail.mit.edu/6.824-2013/notes/l05.txt&#34;&gt;[Lecture Notes] Fault Tolerance: Paxos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://pdos.csail.mit.edu/6.824-2013/papers/paxos-simple.pdf&#34;&gt;[Paper] Paxos Made Simple&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/reading/papers/paxos-simple.html&#34;&gt;My Paxos Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Paxos 键值存储系统包括&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端（ client ）&lt;/li&gt;
&lt;li&gt;kvpaxos 服务器（ server ）&lt;/li&gt;
&lt;li&gt;Paxos 节点（ peers ）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中 Paxos 节点部分以类库的形式运行在服务器之上，在 lab3 的整体模型类似于 &lt;a href=&#34;http://pdos.csail.mit.edu/6.824-2013/papers/paxos-simple.pdf&#34;&gt;Paxos Made Simple&lt;/a&gt; 第三节提到的状态机，其中 proposer、acceptor 和 learner 都是 Paxos 节点，&lt;strong&gt;把整个键值存储服务看成一个状态机&lt;/strong&gt;，它会按照一定的顺序执行客户端发来的所有命令（ Put 请求），每个命令都确定性的把整个状态机向前推进一步。&lt;/p&gt;

&lt;p&gt;在这里，这个状态机是由很多服务器共同构成的，这些服务器之间彼此要&lt;strong&gt;保证执行的命令的顺序完全一致&lt;/strong&gt;，为此，我们维护一个有序序列 *list*，该序列就是所有服务器应当执行命令的顺序。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h1 id=&#34;在服务器收到一个-put-k-v-请求之后:6c805deaca0efd9342e0670f4a49bd04&#34;&gt;在服务器收到一个 Put(k, v) 请求之后&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;在该序列中寻找到当前最小的空位 *list[n]*，也就是新命令应该存储的位置&lt;/li&gt;
&lt;li&gt;向 Paxos 节点 propose 在该位置上放置 &lt;em&gt;key=&amp;gt;value&lt;/em&gt; 这个命令，注意同时可能有多个服务器尝试向 &lt;em&gt;list[n]&lt;/em&gt; 这个位置放置命令，这时 Paxos 负责协调统一，决定最终放置在 &lt;em&gt;list[n]&lt;/em&gt; 处的命令应该是什么&lt;/li&gt;
&lt;li&gt;对于单个服务器而言，如果最终 Paxos 决定放置在 &lt;em&gt;list[n]&lt;/em&gt; 处的命令与其 propose 的相同，则放置命令成功，对 Put 命令的处理结束；反之，则继续尝试在 &lt;em&gt;list[n+1]&lt;/em&gt; 处放置其 propose 的命令。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;How Put(key, value) and px.Start(seq, v) meet?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Log slot?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(* Paxos).Start(seq int, v interface{})&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(* Paxos).Status(seq int) (decided bool, v interface{})&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;part-b:6c805deaca0efd9342e0670f4a49bd04&#34;&gt;Part B&lt;/h2&gt;

&lt;p&gt;在 Part B 中，要在 Paxos 的基础之上实现一个键值存储系统，改系统所能处理的异常情况包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TODO&lt;/li&gt;
&lt;li&gt;TODO&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Part B 中存储系统的结构依然包括客户端、服务端，与 Lab2 中不同的是服务端彼此之间利用 Paxos 来达成共识，因此在该部分中 Paxos 是运行在存储系统服务端之下的辅助系统。下文中提到的客户端、服务端均指存储系统的客户端河服务端。&lt;/p&gt;

&lt;p&gt;###基本工作原理&lt;/p&gt;

&lt;p&gt;服务器本地保存一个请求队列，收到客户端的请求先缓存起来，每次服务器&lt;strong&gt;只处理队列头的请求&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;每个Put请求都要添加一段随机的身份标识，当一个客户端向多个服务器发送请求的时候请求不会被重复处理。&lt;/p&gt;

&lt;p&gt;服务器维护一个列表，表示已经达成共识的消息日志（操作历史），当服务器处理一个请求的时候，尝试添加改请求到列表下一个位置，并通过 Paxos 与其他服务器达成共识。为了解决客户端重复请求的情况，服务器应当先检查本地是否包含改身份标识的操作，确认为新操作之后再进行添加请求。当该请求添加成功之后，处理缓存中下一个请求。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如何确定消息应该添加的位置？&lt;/strong&gt;
由于 Paxos 在收到 decide 信息的时候并不会主动通知上层，因此当请求到来的时候，服务器端保存的消息日志可能只有 n 个 slot，然而底层的 Paxos 达成共识的已经有 n+m 个了。所以服务器在处理请求队列时应当：&lt;strong&gt;1.&lt;/strong&gt; 先同 Paxos 同步信息 &lt;strong&gt;2.&lt;/strong&gt; 根据请求的身份标识判断是否是重复请求 &lt;strong&gt;3.&lt;/strong&gt; 调用 Paxos.Start &lt;strong&gt;4.&lt;/strong&gt; 轮询 Paxos.Status &lt;strong&gt;5.&lt;/strong&gt; 请求占位成功，则返回，失败则返回第1步重新执行。&lt;/p&gt;

&lt;p&gt;服务器端维护的消息日志与 Paxos 节点中维护的消息列表是相对应的，内容也是一致的。不过服务端列表是完整的，而且包含了语义信息，而 Paxos 作为服务系统，并不了解列表中值的具体意义，而且 Paxos 中维护的列表内容是作为缓冲而存在的，一旦服务器同步了 Paxos 列表中的信息，就可以通过 Paxos.Done 来清空 Paxos 中缓存的信息。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Get请求直接从本地读取数据返回？&lt;/strong&gt;
也当成一个操作，请求共识，根据最终决定的操作顺序返回相应的值。&lt;/p&gt;

&lt;p&gt;一种可能的问题：客户端向服务器A发送了Put(a)=1的请求之后，又向服务器B发送Put(a)=2请求。因为Paxos请求并没有一个时间戳，所以可能出现发往B的请求先于发往A的请求达成了共识，造成&lt;strong&gt;客户端的不一致&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notes on Paxos Made Simple</title>
      <link>/post/2014-02-12-paxos/</link>
      <pubDate>Wed, 12 Feb 2014 16:21:02 +0800</pubDate>
      
      <guid>/post/2014-02-12-paxos/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://pdos.csail.mit.edu/6.824-2013/papers/paxos-simple.pdf&#34;&gt;Origin paper link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;problem:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Problem&lt;/h3&gt;

&lt;p&gt;safety requirements&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Only a value that has been proposed may be chosen&lt;/li&gt;
&lt;li&gt;Only a single value is chosen, and&lt;/li&gt;
&lt;li&gt;A process never learns that a value has been chosen unless it
acutally has been&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;three agents: &lt;em&gt;proposers&lt;/em&gt;(P), &lt;em&gt;acceptors&lt;/em&gt;(A), and &lt;em&gt;learners&lt;/em&gt;(L).&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;use customary asynchronous, non-Byzantine model, in which:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agents operate at arbitrary speed, may fail by stopping, and may
restart&lt;/li&gt;
&lt;li&gt;Messages can take arbitrarily long to be delivered, can be duplicated,
and can be lost, but they are not corrupted.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;choosing-a-value:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Choosing a Value&lt;/h3&gt;

&lt;p&gt;Single acceptor, simple but unsatisfacroty, suffer from failure
of this single acceptor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Majority of the agents?&lt;/strong&gt;
Literally understand as more than half of acceptors.&lt;/p&gt;

&lt;p&gt;To ensure that only a single value is chosen, we can let a large
enough set consist of &lt;strong&gt;any majority of the agents&lt;/strong&gt;. Because any two
majorities have at least one acceptor in common, this works if an
acceptor can accept at most one value.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;P1. An acceptor must accept the first proposal that it receives.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This ensure that the value got chosen if there is only one value
proposed. But raises the problem when more than two values are
proposed and each got same amount of acceptors(3 values, each &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of
all acceptors).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Acceptor must be allowed to accept more than one proposals&lt;/strong&gt;
Though there can be only one value that got chosen, but each acceptor indeed could accept more than one proposals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposal number is global? or for each acceptor? How to achieve global?&lt;/strong&gt;
It should be global.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;P2. If a proposal with value &lt;em&gt;v&lt;/em&gt; is chosen, then every higher-numbered
proposal that is chosen has value &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;P2 guarantees the crucial safety property that only a single value is chosen.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;P2a. If a proposal with value &lt;em&gt;v&lt;/em&gt; is chosen, then every higher-numbered
proposal accepted by any acceptor has value &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;P1 may conficts P2a in some situations.
Suppose a proposal was chosen with some particular acceptor &lt;em&gt;c&lt;/em&gt; never
having received any proposal. A new proposer &amp;ldquo;wakes up&amp;rdquo; then and
issues a higher-numbered proposal with a different value. P1 requires
&lt;em&gt;c&lt;/em&gt; to accept this proposal, violating P2a.&lt;/p&gt;

&lt;p&gt;Maintaining both P1 and P2a requires strengthening P2a to:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;P2b . If a proposal with value &lt;em&gt;v&lt;/em&gt; is chosen, then every higher-numbered
proposal issued by any proposer has value &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Difference between concepts &lt;em&gt;chosen&lt;/em&gt;, &lt;em&gt;accept&lt;/em&gt; and &lt;em&gt;issue&lt;/em&gt;?&lt;/strong&gt; &lt;em&gt;Chosen&lt;/em&gt; is a global state that a value &lt;em&gt;v&lt;/em&gt; has been accepted by majority of acceptors, the whole system can only choose one value. &lt;em&gt;Accept&lt;/em&gt; is the behavior of a single acceptor, the acceptor can change its mind to accept another newer proposal at any time. &lt;em&gt;Issue&lt;/em&gt; is the behavior of a single proposer, if a value &lt;em&gt;v&lt;/em&gt; is &lt;em&gt;chosen&lt;/em&gt; (globally accepted), then all proposers would make compromise to propose &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Given that &lt;strong&gt;any two sets of majaority acceptors must have at least one acceptor in common&lt;/strong&gt;. We want the following invariance meet:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;P2c. For any &lt;em&gt;v&lt;/em&gt; and &lt;em&gt;n&lt;/em&gt;, if a proposal with value &lt;em&gt;v&lt;/em&gt; and number &lt;em&gt;n&lt;/em&gt; is
issued, then there is a set &lt;em&gt;S&lt;/em&gt; consisting of a majority of
acceptors such that
either (a) no acceptor in &lt;em&gt;S&lt;/em&gt; has accepted any proposal numbered less
than &lt;em&gt;n&lt;/em&gt;, or (b) &lt;em&gt;v&lt;/em&gt; is the value of the highest-numbered proposal
among all proposals numbered less than &lt;em&gt;n&lt;/em&gt; accepted by the acceptors
in &lt;em&gt;S&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To maintain the invariance of P2c, a proposer that wants to issue a
proposal numbered &lt;em&gt;n&lt;/em&gt; must learn the highest-numbered proposal with
number less than &lt;em&gt;n&lt;/em&gt; that has been or will be accepted by each
acceptor in some majority of acceptors.&lt;/p&gt;

&lt;p&gt;It is hard to predict future acceptances, instead, the proposer controls
it by extracting a &lt;strong&gt;promise&lt;/strong&gt; that the acceptors won&amp;rsquo;t accept any more
proposals numbered less than &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Note that P2c guaranteed that &lt;strong&gt;if a value &lt;em&gt;v&lt;/em&gt; is chosen, then the highest-numbered proposal must have value *v&lt;/strong&gt;*.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm for a proposer to issue proposals&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A proposer choses &lt;em&gt;n&lt;/em&gt;, sends a request to each acceptors in some
set, asking:

&lt;ol&gt;
&lt;li&gt;Promise it won&amp;rsquo;t accept a proposal numbered less than &lt;em&gt;n&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The proposal with highest number less than &lt;em&gt;n&lt;/em&gt; that it has accepted.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The proposer can issue a proposal with number &lt;em&gt;n&lt;/em&gt; and &lt;em&gt;v&lt;/em&gt; if it
receives responses from a majority of the acceptors, where &lt;em&gt;v&lt;/em&gt; is the
value of the highest-numbered proposal among the responses, or is any
value if responders reported no proposals.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The request in step 1 is a &lt;em&gt;prepare&lt;/em&gt; request, and that in step 2 is
an &lt;em&gt;accept&lt;/em&gt; request.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How an acceptor responds to requests?&lt;/strong&gt;
It can always respond to a &lt;em&gt;prepare&lt;/em&gt; request, and it can respond to an
&lt;em&gt;accept&lt;/em&gt; request iff it has not promised not to.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;P1a. An acceptor can accept a proposal numbered &lt;em&gt;n&lt;/em&gt; iff it has not
responded to a prepare request having a number greater than &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An acceptor needs to remember only the highest-numbered proposal that
it has ever accepted and the number of the highest-numbered prepare
request to which it has responded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note that the proposer can always abandon a proposal and forget all about it—as long as it never tries to issue another proposal with the same number. What if a proposer got a promise but never issued?&lt;/strong&gt;
Just like a network package lose. Will be eventually replaced by
other proposals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 1.&lt;/strong&gt;
&lt;strong&gt;(a)&lt;/strong&gt; A proposer selects a proposal number &lt;em&gt;n&lt;/em&gt; and sends a &lt;em&gt;prepare&lt;/em&gt;
request with number &lt;em&gt;n&lt;/em&gt; to a majority of acceptors.
&lt;strong&gt;(b)&lt;/strong&gt; If an acceptor receives a &lt;em&gt;prepare&lt;/em&gt; request with number &lt;em&gt;n&lt;/em&gt; greater
than that of any &lt;em&gt;prepare&lt;/em&gt; request to which it has already responded,
then it responds to the request with a promise not to accept any more
proposals numbered less than &lt;em&gt;n&lt;/em&gt; and with the highest-numbered proposal
(if any) that it has accepted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 2.&lt;/strong&gt;
&lt;strong&gt;(a)&lt;/strong&gt; If the proposer receives a response to its &lt;em&gt;prepare&lt;/em&gt; requests
(numbered &lt;em&gt;n&lt;/em&gt;) from a majority of acceptors, then it sends an &lt;em&gt;accept&lt;/em&gt;
request to each of those acceptors for a proposal numbered &lt;em&gt;n&lt;/em&gt; with a
value &lt;em&gt;v&lt;/em&gt; , where &lt;em&gt;v&lt;/em&gt; is the value of the highest-numbered proposal
among the responses, or is any value if the responses reported no
proposals.
&lt;strong&gt;(b)&lt;/strong&gt; If an acceptor receives an &lt;em&gt;accept&lt;/em&gt; request for a proposal numbered
&lt;em&gt;n&lt;/em&gt;, it accepts the proposal unless it has already responded to a
&lt;em&gt;prepare&lt;/em&gt; request having a number greater than &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: abandon a proposal if some proposers has begun trying to
issue a higher-numbered one.&lt;/p&gt;

&lt;h3 id=&#34;learning-a-chosen-value:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Learning a Chosen Value&lt;/h3&gt;

&lt;p&gt;Acceptors return responds to &lt;em&gt;accept&lt;/em&gt; requests to all the learners,
the number of responds that required equals to the product of the number
of acceptors and the number of learners.&lt;/p&gt;

&lt;p&gt;Acceptors could return only to a set of distinguished learners, these
learners will inform other learners.&lt;/p&gt;

&lt;h3 id=&#34;progress:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Progress&lt;/h3&gt;

&lt;p&gt;It’s easy to construct a scenario in which two proposers each keep
issuing a sequence of proposals with increasing numbers, none of which
are ever chosen.&lt;/p&gt;

&lt;p&gt;To guarantee progress, a distinguished proposer must be selected as the
only one to try issuing proposals.&lt;/p&gt;

&lt;p&gt;Result of &lt;a href=&#34;http://dl.acm.org/citation.cfm?id=214121&#34;&gt;Fischer, Lynch, and Patterson&lt;/a&gt; shows a reliable algorithm
to electing a proposer must use either randomness or real time.&lt;/p&gt;

&lt;h3 id=&#34;implementation:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=279229&#34;&gt;The part-time parliament&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No two proposals are ever issued with the same number?&lt;/strong&gt;
Different proposers choose their numbers from disjoint sets of numbers,
each proposer remembers (in stable storage) the highest-numbered
proposal it has tried to issue.&lt;/p&gt;

&lt;h2 id=&#34;my-summary:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;My Summary&lt;/h2&gt;

&lt;p&gt;Behavior of &lt;em&gt;Proposer&lt;/em&gt;, &lt;em&gt;Acceptor&lt;/em&gt; and &lt;em&gt;Learner&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;proposer:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Proposer&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;sends prepare request &lt;em&gt;n&lt;/em&gt; to all acceptors&lt;/li&gt;
&lt;li&gt;after receives responds from majority of acceptors, choose a value &lt;em&gt;v&lt;/em&gt; according to responds

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;v&lt;/em&gt; should be the value of the highest-numberd proposal in those responds&lt;/li&gt;
&lt;li&gt;if none of responds returns any proposal, use arbitrary value&lt;/li&gt;
&lt;li&gt;if any responds contains an error or a proposal whose number is bigger than &lt;em&gt;n&lt;/em&gt;, go to next step&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;repick a bigger &lt;em&gt;n&lt;/em&gt; and repeat step 1 &amp;amp; 2 until a value is chosen&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;acceptor:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Acceptor&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Acceptor should know the highest-numbered proposal it accepted &lt;em&gt;APa&lt;/em&gt; and the highest-numbered proposal it responsed &lt;em&gt;APr&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Upon receiving a prepare request &lt;em&gt;P&lt;/em&gt;, compare &lt;em&gt;P.n&lt;/em&gt; with &lt;em&gt;APr.n&lt;/em&gt;

&lt;ol&gt;
&lt;li&gt;if &lt;em&gt;P.n&lt;/em&gt; &amp;gt; &lt;em&gt;APr.n&lt;/em&gt;, respond a promise and &lt;em&gt;APa&lt;/em&gt;, then change &lt;em&gt;APr&lt;/em&gt; to &lt;em&gt;P&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;if &lt;em&gt;P.n&lt;/em&gt; &amp;lt;= &lt;em&gt;APr.n&lt;/em&gt;, respond with some error or &lt;em&gt;APr&lt;/em&gt;?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Upon receiving a accept request &lt;em&gt;P&lt;/em&gt;

&lt;ol&gt;
&lt;li&gt;if &lt;em&gt;P.n&lt;/em&gt; &amp;gt;= &lt;em&gt;APr.n&lt;/em&gt;, accept it by making &lt;em&gt;APa&lt;/em&gt; equals to &lt;em&gt;P&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;if &lt;em&gt;P.n&lt;/em&gt; &amp;lt; &lt;em&gt;APr.n&lt;/em&gt;, abondon it&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;learner:6bec04965a0098d4a556c94eb1c1658c&#34;&gt;Learner&lt;/h3&gt;

&lt;p&gt;TODO&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>6.824 Lab2 Notes</title>
      <link>/post/2014-02-08-6-dot-824-lab2-notes/</link>
      <pubDate>Sat, 08 Feb 2014 15:53:24 +0800</pubDate>
      
      <guid>/post/2014-02-08-6-dot-824-lab2-notes/</guid>
      <description>

&lt;p&gt;Lab2要实现一个分布式键-值存储服务（key/value storage service），依然使用primary/backup的架构，支持&lt;strong&gt;Put(key, value)&lt;/strong&gt;和&lt;strong&gt;Get(key)&lt;/strong&gt;两种操作，所有的键-值都存储在P的内存中，不写入硬盘，在正常运行中要保证P/B的状态一致。此外，这个lab要求实现P/B的动态分配，所以在B能正常工作之前要先与P进行状态同步，而且服务器在挂掉之后可能恢复运行，成为新的B。为了使所有人（客户端、所有的服务器）看到的状态（即P/B分别是谁）一致，lab2中引入了视图服务（viewservice）作为一个控制服务器（master server），视图服务负责监控所有键-值服务器的连接状态，在P或B挂掉之后动态的指定新的。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&#34;part-a:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Part A&lt;/h2&gt;

&lt;p&gt;这一部分要实现视图服务，代码文件在/viewservice/文件夹下，命名规则与lab1相同。在partA中，Client的代码已经完整的给出了，我们需要实现的仅仅是Server部分。&lt;/p&gt;

&lt;p&gt;视图服务要实现两个RPC&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ping(Me, Viewnum)&lt;/strong&gt; 键-值服务器通过该方法告诉视图服务：“我还活着，我所知道的当前的视图号是Viewnum”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Get()&lt;/strong&gt; 键-值服务器服务器或其客户端通过该方法向视图服务询问当前最新的视图&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中视图的定义在*common.go*文件中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type View struct {
	Viewnum uint
	Primary string
	Backup  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个&lt;strong&gt;视图（View）&lt;/strong&gt;表征了当前整个键-值存储服务的状态，它表示当前的主要服务器P是Primary，备份服务器B是Backup。每个视图还对应一个视图号（Viewnum），在视图服务初始化的时候视图号为0，以后每次视图发生变化（变更Primary或Backup）该值都会增加1。最开始的时候我以为视图服务要维护视图变更的历史，后来发现这其实是不需要的，视图服务只需要保存当前的视图就可以了。&lt;/p&gt;

&lt;p&gt;视图服务要维护一个包含当前所有键-值服务器的列表，这些服务器中，一个是P，一个是B，其他的都是I（idle，空闲服务器）。所有的键-值服务器每隔100ms（由常量PingInterval定义）就要Ping一下视图服务，如果一个服务器连续5个（由常量DeadPings定义）PingInterval内都没有Ping视图服务，则视图服务&lt;strong&gt;认为该服务器挂掉&lt;/strong&gt;。为了检测距离上一次某个服务器Ping视图服务过了多久，我在视图服务中存储了每个服务器最后一次Ping的时间。&lt;/p&gt;

&lt;p&gt;视图服务在当前视图被P确认之前是不会对视图进行更多的修改的，P确认当前视图的方式是发送一个包含当前视图号的Ping请求（也就是说P已经知道了当前的最新状态）。因此视图服务需要知道当前视图是否被确认了。&lt;/p&gt;

&lt;p&gt;该lab还有另外一个要求，我并没有在文档中看到（也可能是我漏看了=~=），但是在最后一个测试用例中定义了，即为确认当前视图的服务器不能成为P，测试原文是“Uninitialized server can&amp;rsquo;t become primary”，刚开始不清出初始化什么意思，看了测试代码才知道是要对当前的视图进行确认。为了过这一个测试必须知道每个服务器是否初始化了，所以我在记录每个服务器最后一次Ping时间的地方，同时记录了最后一个Ping的视图号。&lt;/p&gt;

&lt;p&gt;总的来讲，视图服务器端我所维护的信息如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type ClientInfo struct {
	name         string    // client&#39;s name
	last_ping    time.Time // Time of last Ping
	last_viewnum uint      // Viewnum of last Ping
	idle         bool      // whether this client is idle
	dead         bool      // whether this client is dead
}

type ViewServer struct {
	mu   sync.Mutex
	l    net.Listener
	dead bool
	me   string

	// Your declarations here.
	current_view      View
	acked             bool // current view is acked by P
	primary_restarted bool // P has restarted within DeadPings * PingInterval

	clients map[string]*ClientInfo // list of k/v servers
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现方面，主要需要实现的函数只有两个，Ping和tick，其中tick是一个私有方法，每隔PingInterval这么久视图服务器就会调用一次该方法。在我的实现中，我选择在Ping中修改服务器列表信息，而在tick中修改视图信息，当然，因为视图的变动而导致的服务器列表信息的变更也是在Tick中实现的。&lt;/p&gt;

&lt;p&gt;以上实现的一个结论就是视图服务视图的更新并不是及时的，这点应该是符合文档的描述的，文档中给了一个例子，但是因为解释的并不是特别清楚所以我并没能一下弄明白这个例子：&lt;/p&gt;

&lt;pre&gt;
Viewnum Primary Backup     Event
--------------------------------
0       none    none
                           server S1 sends Ping(0), which returns view 0
1       S1      none
                           server S2 sends Ping(0), which returns view 1
                             (no view change yet since S1 hasn&#39;t acked)
                           server S1 sends Ping(1), which returns view 1 or 2
2       S1      S2
                           server S1 sends Ping(2), which returns view 2
                           server S1 stops sending Pings
3       S2      none
                           server S2 sends Ping(3), which returns view 3
&lt;/pre&gt;

&lt;!--Viewnum | Primary | Backup | Event
------- | ------- | ------ | ---------
0 |none| none |
  |    |      | server S1 sends Ping(0), which returns view 0
1 | S1 | none |
  |    |      | server S2 sends Ping(0), which returns view 1 (no view change yet since S1 hasn&#39;t acked)
  |    |      | server S1 sends Ping(1), which returns view 1 or 2
2 | S1 |  S2  |
  |    |      | server S1 sends Ping(2), which returns view 2
  |    |      | server S1 stops sending Pings
3 | S2 | none |
  |    |      | server S2 sends Ping(3), which returns view 3
--&gt;

&lt;p&gt;S1向视图服务发送了Ping(0)请求之后服务器并不是立刻更新到视图1的，而是等到下一次tick执行的时候才检查当前视图能否更新，并空闲服务器列表中选出S1作为P。在tick执行之前，S1可能发送了多次Ping(0)给视图服务器，然而只有当tick执行之后，S1发送的Ping(0)请求才会返回view 1。&lt;/p&gt;

&lt;p&gt;文档对这个例子的解释并不完整，比如server S1 sends Ping(0), which returns view 0之后下一次出现S1就是server S1 sends Ping(1), which returns view 1 or 2，最开始看的时候我并不是很理解为啥S1收到view 0的返回值，下次会发送Ping(1)，其实是文档中把返回view 1的那次Ping(0)给省略掉了。这也解释了为啥S1发送Ping(1)会返回view 1 or 2，其实是第一次S1 Ping(1)的时候视图服务只是确认了view 1，还没有转换到view 2，但在确认之后的下一次tick中，视图服务会从空闲服务列表中拎出来一个S2作为B，切换到view 2，这时S1再发送Ping(1)就会返回view 2。&lt;/p&gt;

&lt;p&gt;而在view 2向view 3的切换过程中，S2也不是直接发送了Ping(3)，而是先发送Ping(2)，视图服务返回view 3，S2才知道自己当选了P，并向视图服务发送Ping(3)的。&lt;/p&gt;

&lt;h2 id=&#34;part-b:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Part B&lt;/h2&gt;

&lt;p&gt;这一部分基本思路借鉴Lab1，不再大段总结了，大概阐述下每个函数应该做什么，然后记录下遇到的一些问题。&lt;/p&gt;

&lt;h3 id=&#34;functionality:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Functionality&lt;/h3&gt;

&lt;p&gt;本部分说明各个函数的设计功能，采用函数所在位置（服务端、客户端）加函数命的方式来定位到具体的每个函数。首先客户端、服务端的变量结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Clerk struct {
	vs    *viewservice.Clerk  // 视图服务
	cview viewservice.View    // 当前视图
}

type PBServer struct {
	mu         sync.Mutex
	l          net.Listener
	dead       bool // for testing
	unreliable bool // for testing
	me         string
	vs         *viewservice.Clerk

	// Your declarations here.
	cview  viewservice.View   // 当前视图
	db     map[string]string  // 数据库
	role   string             // 改服务器的角色P/B/I
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外我在原有的常量基础上新增了一个&lt;code&gt;ErrFwdToPrimary&lt;/code&gt;，表示发送了一个Forward请求给当前的P。&lt;/p&gt;

&lt;h4 id=&#34;client-get-put:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Client - Get/Put&lt;/h4&gt;

&lt;p&gt;这两个函数实现基本结构是一样的&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;判断当前客户端看到的视图是否包含P，不包含的话更新当前视图&lt;/li&gt;
&lt;li&gt;向服务端发送请求&lt;/li&gt;
&lt;li&gt;1. 如果服务端没有返回或返回ErrWrongServer，更新视图再次进行步骤2

&lt;ol&gt;
&lt;li&gt;如果服务端返回成功，返回结果&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;server-get:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Server - Get&lt;/h4&gt;

&lt;p&gt;本函数处理客户端发来的Get请求&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;判断自己是不是P，不是返回ErrWrongServer&lt;/li&gt;
&lt;li&gt;根据参数中的Key读取相应的Value，如果Key存在则返回Value，否则返回ErrNoKey&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;server-put:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Server - Put&lt;/h4&gt;

&lt;p&gt;本函数处理客户端发来的Put请求&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;判断自己是不是P，不是返回ErrWrongServer&lt;/li&gt;
&lt;li&gt;如果存在B，Forward请求至B。否则跳至步骤4&lt;/li&gt;
&lt;li&gt;如果B返回ErrFwdToPrimary，则取消Put操作，并返回ErrWrongServer&lt;/li&gt;
&lt;li&gt;更新本地数据库，返回OK&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;server-forward:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Server - Forward&lt;/h4&gt;

&lt;p&gt;本函数处理从P发向B的Fwd请求&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果自己是P，返回ErrFwdToPrimary&lt;/li&gt;
&lt;li&gt;如果自己是I，返回ErrWrongServer&lt;/li&gt;
&lt;li&gt;更新数据库，返回OK&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;server-sync:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Server - Sync&lt;/h4&gt;

&lt;p&gt;本函数处理P发向B的同步请求，在I成为B之后，P会先同B进行一次同步，把自己的数据库复制给B一份，然后再把接下来的Put请求Forward给B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;更新当前视图&lt;/li&gt;
&lt;li&gt;如果自己是P或者I，返回ErrWrongServer&lt;/li&gt;
&lt;li&gt;用参数中的数据库覆盖本地的数据库&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在步骤1中之所以要先更新视图，因为一种可能的情况是P拿到了最新的视图，向B发送Sync请求，但B此时尚未更新，依然认为自己是I，就直接返回ErrWrongServer了。考虑到Sync请求相比Put、Forward而言要少很多，每次Sync都更新下视图是不会成为瓶颈的。&lt;/p&gt;

&lt;h4 id=&#34;server-tick:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Server - tick&lt;/h4&gt;

&lt;p&gt;本函数在服务端每隔一段时间执行一次，功能实现非常简单，调用updateView()函数。&lt;/p&gt;

&lt;h4 id=&#34;server-updateview:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Server - updateView&lt;/h4&gt;

&lt;p&gt;其实完全可以把我这个函数中的内容放在tick里，不过我最初为了把更新当前视图这个功能单独拉出来所以写了这么个函数&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;从视图服务获得最新的视图&lt;/li&gt;
&lt;li&gt;如果当前视图和最新视图一致，直接返回&lt;/li&gt;
&lt;li&gt;更新服务器的role属性&lt;/li&gt;
&lt;li&gt;如果我是P，而新视图中存在B，则进行一次Sync&lt;/li&gt;
&lt;li&gt;更新当前视图&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;问题笔记:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;问题笔记&lt;/h3&gt;

&lt;h4 id=&#34;什么时候需要向视图服务询问当前的视图:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;什么时候需要向视图服务询问当前的视图？&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;每个tick需要&lt;/li&gt;
&lt;li&gt;当P从B那里收到Err&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;当B发现自己成了P，然而之前的P依然向他发送请求的时候，拒绝处理该请求；当P发现同步请求被B拒绝之后，知道自己已经不是P了，询问视图服务&lt;/p&gt;

&lt;h4 id=&#34;b如何区分来自client的put和来自p的put:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;B如何区分来自Client的Put和来自P的Put？&lt;/h4&gt;

&lt;p&gt;使用不同的接口: &lt;code&gt;Put&lt;/code&gt; &lt;code&gt;Forward&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Client是否需要添加Version信息?&lt;/p&gt;

&lt;h4 id=&#34;如何同步状态:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;如何同步状态&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;sync(db map[string]string)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;bug-in-viewservice:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;BUG in viewservice&lt;/h4&gt;

&lt;pre&gt;
Tick() [/var/tmp/824-1000/pb-15185-basic-1]

----role:PRIMARY, view:{V:1, P:/var/tmp/824-1000/pb-15185-basic-1, B:}

Tick() [/var/tmp/824-1000/pb-15185-basic-1]

----role:PRIMARY, view:{V:2, P:/var/tmp/824-1000/pb-15185-basic-1, B:/var/tmp/824-1000/pb-15185-basic-1}
&lt;/pre&gt;

&lt;p&gt;viewservice会把P和B设成同一个Server&lt;/p&gt;

&lt;p&gt;clients list中S1会变成idle，原因是，在tick将S1设成P，但P未ack之前，S1 time out了，所以在之后的tick中会将S1设成idle&amp;amp;dead，但是因为还没有ack，所以server不会将idle&amp;amp;dead的S1从Primary中剔除出去，而是一直等待，直到S1回复运行之后再次正常的Ping，但是此时S1已经被设置成了idle，所以会被选来做B&lt;/p&gt;

&lt;p&gt;这跟我之前考虑到的一个bug类似，同样在S1被选为P，但尚未ack的时候，S1发生了一次crash，并快速恢复了，这时server会检测到一个primary_restarted，但是因为没有ack，所以不会handle，一直要等到S1 ack了之后才会handle，这时S1已经重启了，其内存数据已经丢失&lt;/p&gt;

&lt;h4 id=&#34;syncargs:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;SyncArgs&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;map[string]string&lt;/code&gt; 是指针，不能作为RPC的参数传递。目前使用多个Forward请求来实现Sync，但这样效率极低，对性能影响较大。&lt;/p&gt;

&lt;p&gt;难道是因为我map变量是小写开头的，导致没有export？ client、server因为是同一份文件，编译时没有出现错误。把SyncArgs改为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SyncArgs struct {
    DB map[string]string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过Sync进行同步了，果然是这个原因！&lt;/p&gt;

&lt;h4 id=&#34;primary-would-not-respond-get-after-crash:e1e121a52c11d4a67b50222264b50e8d&#34;&gt;Primary would not respond Get after crash&lt;/h4&gt;

&lt;p&gt;在Get中直接添加对当前身份的判断（pb.role）即可通过测试，但是一个问题是在视图服务判定S1不再是P，但S1收到该视图之前，S1依然认为自己是P，这种简单判断会在这个极段的时间内出现问题&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>